{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Claims Processing\n",
    "this code opens the large downloaded claims file, opens the list of patents numbers and the outcome variable and loops though adding independent claim text and dependent claim count\n",
    "\n",
    "This is really slow and should be sped up, but needs some rewriting to fix this.\n",
    "Also currently I only save a subset of the information, so that could be improved.\n",
    "If I do want to parralellize it, I need to keep in mind how to store order (for dependent claims)\n",
    "Also be conscious of type errors - patent numbers being strings or integers and checking equality\n",
    "\n",
    "Also, since I am somewhat randomly searching the file\n",
    "\n",
    "Note that prior to doing this, I split the file into different files with 10000 lines using \n",
    "`split -l 1000000 claim.tsv`\n",
    "\n",
    "\n",
    "Striped directory `lfs setstripe --count 20 split_claims`"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# when kernel reconnects, make sure code is up to date\n",
    "# save small claims files instead of one big one?\n",
    "# update description with correct file paths\n",
    "# run claims code on engaging\n",
    "# move description and reference files to correct directory\n",
    "# run description and referenceto make sure they work properly with new sample\n",
    "# make sure a check and replace option for processing\n",
    "# rerun with just numerical reference counts"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "import os\n",
    "import csv\n",
    "import concurrent\n",
    "import pandas as pd\n",
    "import numpy as np\n",
    "import gzip\n",
    "import pickle\n",
    "\n",
    "import funcy"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "100000000"
      ]
     },
     "execution_count": 2,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "csv.field_size_limit(100000000)\n",
    "# need to do this because claims file loop kept stopping because of the error due to fields longer than max field size\n",
    "# I deal with really long fields by only taking the first 2000 characters or so\n",
    "csv.field_size_limit()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Split description directory\n",
    "index_file = '/pool001/lraymond/patent_data/index_files/pat_nums_index.csv'\n",
    "TO_SAVE_INDEX_FILE = '/pool001/lraymond/patent_data/index_files/pat_nums_plus_claims.gzip'\n",
    "CLAIMS_FILES_DIR = '/nobackup1/lraymond/patent_data/split_claims'\n",
    "TO_SAVE_CLAIMS_DIR = '/pool001/lraymond/processed_data/claims'\n",
    "# I should be story these files in the lustre parallel file system\n",
    "# '/nobackup1/lraymond/patent_data/split_brief_description'\n",
    "# I want to stripe this directory over 20 servers to optimize performance \n",
    "# lfs setstripe --count 20 patent_data/split_brief_description\n",
    "# old_dir on laptop = '/Users/Lraymond/Python/patents/data/patent_api_data/split_test'"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/home/software/sloan/local/lib/py36/IPython/core/interactiveshell.py:2785: DtypeWarning: Columns (9) have mixed types. Specify dtype option on import or set low_memory=False.\n",
      "  interactivity=interactivity, compiler=compiler, result=result)\n"
     ]
    }
   ],
   "source": [
    "# pats_index = load_zipped_pickle(index_file)\n",
    "pats_index = pd.read_csv(index_file)\n",
    "\n",
    "pats_index = pats_index.sort_values('patent_number')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [],
   "source": [
    "def save_zipped_pickle(obj, filename, protocol=-1):\n",
    "    # this will default to the highest protocol\n",
    "    with gzip.open(filename, 'wb') as f:\n",
    "        pickle.dump(obj, f, protocol)\n",
    "\n",
    "\n",
    "def load_zipped_pickle(filename):\n",
    "    # load a zipped compressed pickle file\n",
    "    with gzip.open(filename, 'rb') as f:\n",
    "        loaded_object = pickle.load(f)\n",
    "        return loaded_object\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [],
   "source": [
    "# save the patent text in a dictionary\n",
    "def create_claims_dicts(patent_nums):\n",
    "    # create independnet and dependent claims dictionaries\n",
    "    return dict(zip(patent_nums, funcy.repeat(None)))\n",
    "\n",
    "def create_claim_dataframe(patent_nums):\n",
    "    nums = pd.Series(patent_nums).astype(int).drop_duplicates().copy()\n",
    "    patent_random_sample = pd.DataFrame(0, \n",
    "        index=nums, columns=['max_independent_claim',\n",
    "                             'number_independent_claims_found',\n",
    "                             'number_dependent_claims_found'])\n",
    "    return patent_random_sample"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [],
   "source": [
    "def save_independent_claims(patent_num, txt, seq, to_save_dir=TO_SAVE_CLAIMS_DIR):\n",
    "    fname = os.path.join(to_save_dir, 'claims_{}.gzip'.format(patent_num))\n",
    "    if os.path.exists(fname):\n",
    "        # append claim to \n",
    "        existing = load_zipped_pickle(fname)\n",
    "        existing.append((txt, seq))\n",
    "        e2 = sorted(existing, key=lambda x: x[1])\n",
    "    else:\n",
    "        e2 = [(txt, seq)]\n",
    "    save_zipped_pickle(e2, fname)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [],
   "source": [
    "def process_independent_claim(patent_num, txt, seq, text_claim_dict, numerical_claim_df):\n",
    "    # save text to the text dictionary\n",
    "    # save independent and dependent claim text to the text dictionaries\n",
    "#     if text_claim_dict[patent_num] is None:\n",
    "#         text_claim_dict[patent_num] = [(txt, seq),]\n",
    "#     else:\n",
    "#         text_claim_dict[patent_num].append((txt, seq))\n",
    "    save_independent_claims(patent_num, txt, seq)\n",
    "    # update dataframe with max sequence of independent claim and number found\n",
    "    existing = numerical_claim_df.loc[patent_num, 'max_independent_claim']\n",
    "    numerical_claim_df.loc[patent_num, 'max_independent_claim'] = max(existing, seq)\n",
    "    numerical_claim_df.loc[patent_num, 'number_independent_claims_found'] += 1\n",
    "    return None\n",
    "\n",
    "def process_dependent_claim(patent_num, txt, seq, claim_dependence, text_claim_dict, numerical_claim_df):\n",
    "    # save text to the text dictionary\n",
    "    # save independent and dependent claim text to the text dictionaries\n",
    "    if text_claim_dict[patent_num] is None:\n",
    "        text_claim_dict[patent_num] = [(txt, seq, claim_dependence),]\n",
    "    else:\n",
    "        text_claim_dict[patent_num].append((txt, seq, claim_dependence))\n",
    "        # because all depednent claims have a NULL field for claim dependence, not putting a lot of stock in them\n",
    "    # update dataframe with max sequence of independent claim and number found\n",
    "#     existing = numerical_claim_df.loc[patent_num, 'max_dependent_claim']\n",
    "#     numerical_claim_df.loc[patent_num, 'max_dependent_claim'] = max(existing, seq)\n",
    "    numerical_claim_df.loc[patent_num, 'number_dependent_claims_found'] += 1\n",
    "    return None\n",
    "\n",
    "\n",
    "def check_get_independent_claim(claim_dependence_str):\n",
    "    # if claim is independent, this should be -1, but value comes in as a string and str.isdigit() returns False for -1\n",
    "    if claim_dependence_str is None or len(claim_dependence_str) == 0:\n",
    "        return False, None\n",
    "    if claim_dependence_str == '-1':\n",
    "        return True, -1\n",
    "    valid_int = claim_dependence_str.isdigit()\n",
    "    if valid_int:\n",
    "        return False, int(claim_dependence_str)\n",
    "    return False, None\n",
    "        # this is an independent claim"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Known Issues with Claims Files\n",
    "* the claim dependence field is \"Null\" for a bunch of claims\n",
    "* I am not including these in either the independent or dependent claim field at the moment\n",
    "* there are only a subset of numbers where this is an issue- they all have \"Null\" in the independent claim field and the claim text is as follows\n",
    "`4785101\t3,4-Dihydro-6-[5-(3-hydroxy-pyrazolyl)]-2(1H)-quinazolinone or a pharmaceuticlaly acceptable salt thereof.\tNULL\t18\tFalse`\n",
    "* I will mark these as dependent and process them as that for now\n",
    "* patent numbers with these issues are \n",
    "`{4581354, 4658060, 4659762, 4748238, 4785101, 4797495, 4879313, 4996207,  5053511, 5294612}`"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [],
   "source": [
    "def yield_file_lines(\n",
    "    filename, list_patent_nums, valid_row_len, max_lines=None, patent_number_index=1):\n",
    "    # create iterator to process file\n",
    "    # create set to check membership\n",
    "    # only returns lines with patent numbers in the set and those with a valid number of fields\n",
    "    # row is returned as a list with a length equal to the number of fields available\n",
    "    set_patent_nums = set(list_patent_nums)\n",
    "    with open(filename) as tsvin:\n",
    "        tsvin = csv.reader(tsvin, delimiter='\\t', quoting=csv.QUOTE_NONE)\n",
    "        for row in tsvin:\n",
    "            if max_lines and tsvin.line_num > max_lines:\n",
    "                # if a max line limit, break out of the generator\n",
    "                return\n",
    "            else:\n",
    "                # only process if the row has all fields\n",
    "                if len(row) == valid_row_len:\n",
    "                    patent_num_field = row[patent_number_index]\n",
    "                    is_num = patent_num_field.isdigit()\n",
    "\n",
    "                    if is_num and int(patent_num_field) in set_patent_nums:\n",
    "                        # only yield relevant lines\n",
    "                        yield row"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [],
   "source": [
    "def process_claims_information(claims_filename, patent_nums_list,\n",
    "                               max_lines=None, claim_files_dir=CLAIMS_FILES_DIR):\n",
    "    # ind_dict = create_claims_dicts(patent_nums_list)\n",
    "    # just processing and saving claims, \n",
    "    ind_dict = {}\n",
    "\n",
    "    claim_numerical_df = create_claim_dataframe(patent_nums_list)\n",
    "    claims_fname = os.path.join(claim_files_dir, claims_filename)\n",
    "    try:\n",
    "        for line in yield_file_lines(\n",
    "            claims_fname, patent_nums_list, 6, max_lines, patent_number_index=1):\n",
    "            # claim dependence is sequence number of claim this is dependent on. -1 if independent\n",
    "            # sequence is order in which claims appear in patent file\n",
    "            # exemplary, the last field is whether the claim is one of the exemplary claims of the patent\n",
    "            # these appear to be claims that include the words \"for example\" but not totally sure about this\n",
    "            \n",
    "            _, str_pat_num, txt, claim_dependence_str, seq_str, _ = line\n",
    "            patent_num = int(str_pat_num)\n",
    "            # something is going wrong with this line\n",
    "            is_independent, claim_dependence = check_get_independent_claim(claim_dependence_str)   \n",
    "            seq = int(seq_str) if seq_str.isdigit() else None\n",
    "            # claim dependence is -1 for independent claims\n",
    "            if is_independent:\n",
    "                process_independent_claim(patent_num, txt, seq, ind_dict, claim_numerical_df)\n",
    "                # for now, I am going to skip processing dependent claims\n",
    "#             else:\n",
    "#                 # this includes cases when seq is nan\n",
    "#                 process_dependent_claim(patent_num, txt, seq, claim_dependence, dep_dict, claim_numerical_df)\n",
    "    except (Exception, KeyboardInterrupt) as e:\n",
    "        print(e)\n",
    "    finally:\n",
    "        # always execute this piece of code even after an exception\n",
    "        return claim_numerical_df, _"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 22,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "['xak', 'xcn', 'xbd', 'xck', 'xcl', 'xdq', 'xbu', 'xbl', 'xbf', 'xbs', 'xdj', 'xbz', 'xar', 'xbk', 'xai', 'xdm', 'xam', 'xcp', 'xde', 'xcz', 'xal', 'xcg', 'xcf', 'xcr', 'xcm', 'xbb', 'xbe', 'xaa', 'xaf', 'xas', 'xcs', 'xac', 'xcw', 'xbi', 'xdd', 'xbv', 'xaz', 'xdb', 'xbo', 'xbg', 'xbw', 'xdh', 'xay', 'xba', 'xbt', 'xby', 'xbj', 'xab', 'xbh', 'xag', 'xaq', 'xao', 'xca', 'xco', 'xdc', 'xdl', 'xdi', 'xbq', 'xbx', 'xav', 'xaj', 'xcu', 'xah', 'xcj', 'xan', 'xcy', 'xae', 'xdf', 'xcb', 'xau', 'xct', 'xaw', 'xax', 'xdp', 'xbp', 'xdk', 'xat', 'xbm', 'xce', 'xcd', 'xbc', 'xdg', 'xap', 'xcx', 'xda', 'xcv', 'xdn', 'xbr', 'xci', 'xad', 'xch', 'xbn', 'xdo', 'xcq', 'xcc'] (1994958,)\n"
     ]
    }
   ],
   "source": [
    "# get list of patent numbers to scrape\n",
    "patent_nums_list = pats_index.patent_number.values\n",
    "txt_files = os.listdir(CLAIMS_FILES_DIR)\n",
    "print(txt_files, patent_nums_list.shape)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# in returned item, first is a dataframe\n",
    "dict_results = dict(zip(txt_files, map(\n",
    "    lambda x: process_claims_information(x, patent_nums_list), txt_files)))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def concat_dfs(dict_results):\n",
    "    # combine dataframes by adding a column with filename source\n",
    "    # then reset index so patent number is a column, append all df together\n",
    "    # then summarize by apply max/sum functions to respective  columns\n",
    "    # process dataframe associated with the results\n",
    "    list_df = []\n",
    "    for fname, res in dict_results.items():\n",
    "        # get dataframe\n",
    "        mini_df = res[0]\n",
    "        print(fname)\n",
    "        mini_df['source_filename'] = fname\n",
    "        mini_df = mini_df.reset_index(drop=False)\n",
    "        mini_df = mini_df.rename(columns={'index': 'patent_number'})\n",
    "        list_df.append(mini_df)\n",
    "    # memory concerns so deleting dict results\n",
    "    del dict_results\n",
    "    return pd.concat(list_df, axis=0, join='outer', ignore_index=True)\n",
    "\n",
    "\n",
    "def find_filenames_with_info(patent_num, ungrouped_df):\n",
    "    filenames = ungrouped_df.loc[\n",
    "        ((ungrouped_df.patent_number==patent_num) & (\n",
    "            ungrouped_df['number_independent_claims_found'] > 0)), 'source_filename'].values\n",
    "    if len(filenames) > 0:\n",
    "        return ' ; '.join(filenames)\n",
    "    return np.nan"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "master_df = concat_dfs(dict_results)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "master_df2 = master_df.sort_values(['patent_number', 'source_filename']).groupby('patent_number').agg(\n",
    "    {'max_independent_claim': np.max, \n",
    "     'number_independent_claims_found': np.sum,\n",
    "    'number_dependent_claims_found': np.sum})"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "master_df2['independent_claims_filenames'] = np.nan\n",
    "filled_vals_mask = (master_df2.number_independent_claims_found > 0) \n",
    "\n",
    "master_df2.loc[filled_vals_mask, 'independent_claims_filenames'] = master_df2.loc[\n",
    "    filled_vals_mask,:].index.map(\n",
    "        lambda x: find_filenames_with_info(x, master_df))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "master_df3 = master_df2.reset_index(drop=False).drop_duplicates('patent_number')\n",
    "\n",
    "print(master_df.shape)\n",
    "print(master_df2.shape)\n",
    "print(master_df3.shape)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "del master_df\n",
    "del master_df2"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "pats2 = pats_index.sort_values(\n",
    "    'patent_number').merge(master_df3, on='patent_number', how='left')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "pats2[\n",
    "    'flag_has_independent_claims'] = pats2.number_independent_claims_found.apply(\n",
    "        lambda x: int(x>0))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "print(pats2.shape)\n",
    "print(pats2.drop_duplicates('patent_number').shape)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "save_zipped_pickle(pats2, TO_SAVE_INDEX_FILE) "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.6.3"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
